{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset by Prajna Bhandary\n",
    "#reference from pyimagesearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imutils\n",
    "#import argparse\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, Dropout, AveragePooling2D\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1315 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = ImageDataGenerator(rescale = 1.0/255,\n",
    "                                  rotation_range = 40,\n",
    "                                  width_shift_range= 0.2,\n",
    "                                  height_shift_range = 0.2,                                  \n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2,\n",
    "                                  horizontal_flip=True,\n",
    "                                  fill_mode='nearest')\n",
    "\n",
    "train_data = train_generator.flow_from_directory('./train',\n",
    "                                                   batch_size = 10,\n",
    "                                                   target_size = (224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 194 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = ImageDataGenerator(rescale = 1.0/255)\n",
    "test_data = test_generator.flow_from_directory('./test',\n",
    "                                               batch_size = 10,\n",
    "                                               target_size = (224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 15\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "#using MobileNetV2\n",
    "\n",
    "baseModel = MobileNetV2(weights='imagenet', include_top=False,\n",
    "                       input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "baseModel.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    baseModel,\n",
    "    \n",
    "    AveragePooling2D(pool_size=(7,7)),\n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128, activation = \"relu\"),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "#opt = Adam(lr=learning_rate, decay=learning_rate / epochs)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-ba501a4f2ae1>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/15\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.1308 - accuracy: 0.9445WARNING:tensorflow:From c:\\mysoftwares\\python3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From c:\\mysoftwares\\python3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model_01.model\\assets\n",
      "132/132 [==============================] - 161s 1s/step - loss: 0.1308 - accuracy: 0.9445 - val_loss: 0.0434 - val_accuracy: 0.9794\n",
      "Epoch 2/15\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9787INFO:tensorflow:Assets written to: model_02.model\\assets\n",
      "132/132 [==============================] - 146s 1s/step - loss: 0.0657 - accuracy: 0.9787 - val_loss: 0.0121 - val_accuracy: 0.9948\n",
      "Epoch 3/15\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0622 - accuracy: 0.9764INFO:tensorflow:Assets written to: model_03.model\\assets\n",
      "132/132 [==============================] - 139s 1s/step - loss: 0.0622 - accuracy: 0.9764 - val_loss: 0.0095 - val_accuracy: 0.9948\n",
      "Epoch 4/15\n",
      "132/132 [==============================] - 100s 758ms/step - loss: 0.0440 - accuracy: 0.9871 - val_loss: 0.0235 - val_accuracy: 0.9897\n",
      "Epoch 5/15\n",
      "132/132 [==============================] - 92s 698ms/step - loss: 0.0329 - accuracy: 0.9901 - val_loss: 0.0271 - val_accuracy: 0.9897\n",
      "Epoch 6/15\n",
      "132/132 [==============================] - 82s 620ms/step - loss: 0.0384 - accuracy: 0.9871 - val_loss: 0.0125 - val_accuracy: 0.9897\n",
      "Epoch 7/15\n",
      "132/132 [==============================] - 82s 625ms/step - loss: 0.0334 - accuracy: 0.9871 - val_loss: 0.0120 - val_accuracy: 0.9948\n",
      "Epoch 8/15\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0324 - accuracy: 0.9886INFO:tensorflow:Assets written to: model_08.model\\assets\n",
      "132/132 [==============================] - 145s 1s/step - loss: 0.0324 - accuracy: 0.9886 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
      "Epoch 9/15\n",
      "132/132 [==============================] - 81s 613ms/step - loss: 0.0324 - accuracy: 0.9894 - val_loss: 0.0118 - val_accuracy: 0.9897\n",
      "Epoch 10/15\n",
      "132/132 [==============================] - 81s 614ms/step - loss: 0.0272 - accuracy: 0.9932 - val_loss: 0.0265 - val_accuracy: 0.9897\n",
      "Epoch 11/15\n",
      "132/132 [==============================] - 80s 608ms/step - loss: 0.0434 - accuracy: 0.9886 - val_loss: 0.0151 - val_accuracy: 0.9948\n",
      "Epoch 12/15\n",
      "132/132 [==============================] - 83s 629ms/step - loss: 0.0315 - accuracy: 0.9871 - val_loss: 0.0085 - val_accuracy: 0.9948\n",
      "Epoch 13/15\n",
      "132/132 [==============================] - 81s 615ms/step - loss: 0.0297 - accuracy: 0.9901 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9901INFO:tensorflow:Assets written to: model_14.model\\assets\n",
      "132/132 [==============================] - 146s 1s/step - loss: 0.0190 - accuracy: 0.9901 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "132/132 [==============================] - 83s 630ms/step - loss: 0.0197 - accuracy: 0.9909 - val_loss: 0.0072 - val_accuracy: 0.9948\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model_{epoch:02d}.model', \n",
    "                            monitor = 'val_loss',\n",
    "                            verbose = 0,\n",
    "                            save_best_only=True,\n",
    "                            mode='auto')\n",
    "\n",
    "history = model.fit_generator(train_data,\n",
    "                             epochs=15,\n",
    "                             validation_data=test_data,\n",
    "                             callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"best_model.model\", save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
